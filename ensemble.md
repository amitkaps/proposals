# The Power of Ensembles

## Overview

Creating better models is a critical component to building a good data science product. It is relatively easy to build a first-cut machine-learning model, but what does it take to build a reasonably good or state-of-the-art model? Ensemble models—which help exploit the power of computing in searching the solution space. The speakers discuss various strategies to build ensemble models.


## Detailed Abstract

Ensemble methods aren’t new. They form the basis for some extremely powerful machine learning algorithms like random forests and gradient boosting machines. The key point about ensemble is that consensus from diverse models are more reliable than a single source. It is indeed hardly surprising that the winning models in [Kaggle](https://kaggle.com) these are ensemble models.


The speakers discuss various strategies to build ensemble models, demonstrating how to combine model outputs from various base models (logistic regression, decision trees, random forest, GBM, neural networks, etc.) to create a stronger, better model output. Using an example package the authors created with bindings to Python([link](https://github.com/unnati-xyz/ensemble-package)), the speakers cover bagging, boosting, stacking, and blending and draw on real-life examples from the enterprise world to explore where ensemble models can consistently produce better results when compared against the best-performing single models.  

Topics include:
- Feature engineering 
- Model selection
- The importance of bias-variance and generalization


By the end of this talk, the attendees will learn what ensemble models are and when and how to implement ensemble models for their problems. We hope it inspires the attendees to go and win a Kaggle competition !

A version of this talk was presented at Strata Singapore 2016. Slides can be found [here](https://www.slideshare.net/amitkaps/the-power-of-ensembles-in-machine-learning)
