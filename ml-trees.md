# From Trees to Forest

## Description
Decision trees are the simplest, yet the most powerful learning machine learning algorithm. They are intuitive to understand, can be visualised and perform well for large set of data problems. More importantly, trees are a construct that most programmers are familiar with as data structures. This talks builds the link between trees structure in programming to decision trees in machine learning. In this talk, I will use  python code and visualisation to share how concepts of growing, walking, pruning, grafting and aggregating trees can be applied to data to build your machine learning models.

## Who and Why (Audience)
Trees are widely used abstract data structure that most programmers are familiar with. Tree enumeration, traversal, adding, pruning, grafting are simple programs that many have written in their coding learning journey. Decision Trees is the equivalent powerful construct in machine learning which can be understood using the same principles. This talks aims to provide a intuitive understanding of the core principles behind decision trees by building an analogy and help programmers interested in data science undertake their first steps in data science.

## Outline

**Context**
Many python programmers are interested to build data application and start learning the principles of machine learning to do so. And Python has an excellent ecosystem of libraries like numpy, pandas, matplotlib, sci-kit which are well documented and excellent starts to do data wrangling, visualisation and build machine learning models. However, the math required in terms of understanding linear algebra, calculus and statistics are actually a huge challenge in making this start. Most machine learning courses start with teaching linear regression and logistic regression (for classification) which can require a fair bit of knowledge of the math to get started.

However, in practice Decision Trees and enhancement built on it - Random Forest and Gradient Boosting are the most powerful and successful algorithms in practice for many typical data problems in business (especially for tabular data). Decision Trees have many advantages - they are intuitive, simple to understand, can be visualised easily, don't require data normalisation etc. I believe learning Decision Trees in a visual and code driven way is a great start for machine learning and ideal for many python programmers.

Also, more importantly learning happens when we can take a new concept and relate it to existing concepts that we are already exposed to. We are then in a position to build from a conceptual mental model and bring it to a new domain. In this particular case, Decision Tree have a strong parallel with Binary Tree Data Structure that many programmers are already exposed to. 

This talks aim to show the parallel between Tree Data Structure and Decision Tree. It illustrates how concepts of Tree Data Structure - Search, Traversal, Adding, Pruning, Grafting and Aggregating have direct congruence in Decision Tree. And how you can use that mental model to jump start your learning.

A singular exempler will be used across the talk to communicate this concept. The python code will be intuitive and simple and will use core scientific python libraries like numpy and core visualisation libraries like matplotlib to show this.

**Outline for the Talk**

1. **Tree Data Structure** (4 mins)
    - Node and Leaf
    - Depth and Degree
    - Edge and Path
    - Tree and Forest

2. **Decision Trees** (4 mins)
    - Binary Decision Trees
    - Search in Tree Structures
    - Classification Problems in Decision Tree

3. **Growing a Tree** (4 mins)
    - Adding nodes to Tree
    - Partitioning in Decision Trees

4. **Pruning a Tree** (4 mins)
   - Measuring Complexity in Trees
   - Error metrics in Decision Trees
 
5. **Walking a Tree** (4 mins)
   - Tree Traversal Methods
   - Making Predictions in Decision Trees
 
6. **Grafting a Tree** (4 mins) 
    - Enhancing a Tree by adding nodes
    - Gradient Boosting in Decision Trees

7. **Making a Forest** (4 mins)
    - Aggregating many trees
    - Bootstrap Aggregation and Random Forest

8. **Generalisation** (2 mins)
    - Additional concepts in Trees
    - Intuition for moving beyond Trees


## Additional Notes

The speaker has been involved in doing analytics and visualisation for more than fifteen years and has been actively involved in teaching Data Visualisation and Data Science for the last five years. You can see his speaking profile at [http://amitkaps.com/talks](http://amitkaps.com/talks)

He has been teaching Machine Learning over the last two year. You can browse through the workshop material on decision trees at the GitHub repo - https://github.com/amitkaps/trees

*Select Talks and Workshops in the Last Two Year*
- Fifth Elephant 2016 (India's biggest data science conference): Workshop on HackerMath for Machine Learning 
- Strata Singapore 2016: The Power of Ensembles - http://www.slideshare.net/amitkaps/the-power-of-ensembles-in-machine-learning
- Strata Singapore 2016: Deep Learning for NLP - http://www.slideshare.net/amitkaps/deep-learning-for-nlp-69972908 
- Strata New York 2016: Visualising Machine Learning Models -  https://www.youtube.com/watch?v=xqAbfMgmBas
- PyCon Singapore 2016: Workshop on Data Analysis and Machine Learning
- Fifth Elephant 2015: Visualising Multi-Dimensional Data - https://www.youtube.com/watch?v=X8rNDvPNg30


